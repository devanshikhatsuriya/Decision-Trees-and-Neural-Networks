#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import pandas
import numpy
import time
import sklearn
import matplotlib.pyplot as plt

import sys


# In[2]:

if len(sys.argv) != 4:
	print("Please provide relative or absolute <path_of_train_data>, <path_of_test_data> and <path_of_validation_data> as command line arguments.")
	sys.exit()

train_path = sys.argv[1]
test_path = sys.argv[2]
val_path = sys.argv[3]

# train_dataframe = pandas.read_csv('../bank_dataset/bank_train.csv', sep=';', header=0)
# test_dataframe = pandas.read_csv('../bank_dataset/bank_test.csv', sep=';', header=0) 
# val_dataframe = pandas.read_csv('../bank_dataset/bank_val.csv', sep=';', header=0)
try:
	train_dataframe = pandas.read_csv(train_path, sep=';', header=0)
	test_dataframe = pandas.read_csv(test_path, sep=';', header=0) 
	val_dataframe = pandas.read_csv(val_path, sep=';', header=0)
except:
	print("Error: Incorrect path for data")
	sys.exit()


# In[ ]:


col_names = list(train_dataframe.columns) 
attr_names = col_names[:-1] # remove the prediction y column name
unique_values = [[] for i in range(len(col_names))]
n_attrs = len(attr_names)

for i in range(len(col_names)):
    unique_values[i] = sorted(train_dataframe[col_names[i]].unique())
    
attr_category = [] # 0 for real-valued, 1 for boolean, 2 for categorical
for i in range(len(attr_names)):
    if len(unique_values[i]) <= 2:
        attr_category.append(1) # boolean-valued attribute
    elif isinstance(unique_values[i][0], str):
        if len(unique_values[i]) <= 2:
            attr_category.append(1) # boolean-valued attribute
        else:
            attr_category.append(2) # categorical attribute
    else:
        attr_category.append(0) # real-valued attribute


# In[ ]:


# convert categorical attributes with integer values to one-hot encodings
# convert yes/no attributes to 1/0
train_df_new = train_dataframe
test_df_new = test_dataframe
val_df_new = val_dataframe
for i in range(len(attr_names)):
    attr = attr_names[i]
    if attr_category[i] == 2:
        one_hot = pandas.get_dummies(train_df_new[attr], prefix=attr) # one-hot encodings
        train_df_new = train_df_new.drop(attr, axis=1)
        train_df_new = train_df_new.join(one_hot)
        one_hot = pandas.get_dummies(test_df_new[attr], prefix=attr) # one-hot encodings
        test_df_new = test_df_new.drop(attr, axis=1)
        test_df_new = test_df_new.join(one_hot)
        one_hot = pandas.get_dummies(val_df_new[attr], prefix=attr) # one-hot encodings
        val_df_new = val_df_new.drop(attr, axis=1)
        val_df_new = val_df_new.join(one_hot)
    elif attr_category[i] == 1:
        train_df_new[attr] = (train_df_new[attr] == unique_values[i][1]).astype(int)
        test_df_new[attr] = (test_df_new[attr] == unique_values[i][1]).astype(int)
        val_df_new[attr] = (val_df_new[attr] == unique_values[i][1]).astype(int)
# convert y labels from yes/no to 1/0
y = (train_df_new['y'] == unique_values[-1][1]).astype(int)
train_df_new = train_df_new.drop('y', axis=1)
train_df_new = train_df_new.join(y)
y = (test_df_new['y'] == unique_values[-1][1]).astype(int)
test_df_new = test_df_new.drop('y', axis=1)
test_df_new = test_df_new.join(y)
y = (val_df_new['y'] == unique_values[-1][1]).astype(int)
val_df_new = val_df_new.drop('y', axis=1)
val_df_new = val_df_new.join(y)


# In[ ]:


# analysing sensitivity of random forest model


# In[ ]:


data = train_df_new.to_numpy()
X = data[:, 0:-1]
y = data[:, -1]
test_data = test_df_new.to_numpy()
test_X = test_data[:, 0:-1]
test_y = test_data[:, -1]
val_data = val_df_new.to_numpy()
val_X = val_data[:, 0:-1]
val_y = val_data[:, -1]


# In[ ]:

from sklearn.ensemble import RandomForestClassifier


params = ['n_estimators', 'max_features', 'min_samples_split']
param_grid = [list(range(50, 451, 100)), [0.1, 0.3, 0.5, 0.7, 0.9], list(range(2, 11, 2))]

# load cached best parameters generated by 1(c)
best_parameters = numpy.loadtxt('1c_best_parameters.csv', delimiter=',')
best_param_dict = {params[j]: best_parameters[j] for j in range(3)}

for i in range(len(params)):
    
    param = params[i] # parameter to vary
    print(f"Varying parameter {param}...")
    
    test_accs = []
    val_accs = []
    
    param_dict = {params[j]: best_parameters[j] for j in range(3)}
    
    for value in param_grid[i]:
        # train model
        param_dict[param] = value
        rf = RandomForestClassifier(criterion="entropy", bootstrap=True, oob_score=True, verbose=1, n_estimators=int(param_dict['n_estimators']), max_features=param_dict['max_features'], min_samples_split=int(param_dict['min_samples_split']))
        print(f"\tFitting random forest using {param} = {value}...")
        start = time.process_time()
        rf.fit(X, y)
        print(f"\tTime taken: {time.process_time() - start} seconds")
        print("\tComputing test and validation accuracies...")
        test_acc = rf.score(test_X, test_y)
        val_acc = rf.score(val_X, val_y)
        test_accs.append(test_acc)
        val_accs.append(val_acc)
        
    print("Generating accuracy vs. value plot...")
    
    plt.figure()
    fig = plt.gcf()
    fig.set_size_inches(8, 6)

    plt.plot(param_grid[i], test_accs, label="Test Accuracy")
    plt.plot(param_grid[i], val_accs, label="Validation Accuracy")
    plt.axvline(x=best_param_dict[param], color='k', ls='--', label=f"Best {param} Value")
    plt.xlabel(param)
    plt.ylabel("Accuracies")
    # plt.title(f"Accuracy on varying {param}")
    plt.legend()
    plt.show()

    fig_name = "1d_" + param +'.png'
    fig.savefig(fig_name, dpi=100)
    
    print(f"Plot saved as {fig_name}...")


# In[ ]:




